{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liching\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\", with_info= True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/plain_text/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset.\n",
      "    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Plain text\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_path='~\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=129.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
       " 'test': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
       " 'unsupervised': <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for i in imdb['train'].take(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the train and test sets\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "# Initialize sentences and labels lists\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "# Loop over all training examples and save the sentences and labels\n",
    "for s,l in train_data:\n",
    "  training_sentences.append(s.numpy().decode('utf8'))\n",
    "  training_labels.append(l.numpy())\n",
    "\n",
    "# Loop over all test examples and save the sentences and labels\n",
    "for s,l in test_data:\n",
    "  testing_sentences.append(s.numpy().decode('utf8'))\n",
    "  testing_labels.append(l.numpy())\n",
    "\n",
    "# Convert labels lists to numpy array\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "vocab_size = 10000\n",
    "max_length = 120\n",
    "embedding_dim = 16\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary for the training sentences\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Generate and pad the training sequences\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# Generate and pad the test sequences\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 120, 16)           160000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1920)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 11526     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Setup the training parameters\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 11s 8ms/step - loss: 0.4840 - accuracy: 0.7519 - val_loss: 0.3936 - val_accuracy: 0.8215\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.2331 - accuracy: 0.9098 - val_loss: 0.4120 - val_accuracy: 0.8196\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.0826 - accuracy: 0.9794 - val_loss: 0.5110 - val_accuracy: 0.8104\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.0196 - accuracy: 0.9976 - val_loss: 0.5965 - val_accuracy: 0.8086\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.0049 - accuracy: 0.9997 - val_loss: 0.6781 - val_accuracy: 0.8056\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7257 - val_accuracy: 0.8099\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 7.7197e-04 - accuracy: 1.0000 - val_loss: 0.7754 - val_accuracy: 0.8098\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 4.3016e-04 - accuracy: 1.0000 - val_loss: 0.8195 - val_accuracy: 0.8100\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 2.5778e-04 - accuracy: 1.0000 - val_loss: 0.8647 - val_accuracy: 0.8093\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 1.4896e-04 - accuracy: 1.0000 - val_loss: 0.9025 - val_accuracy: 0.8097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cab1681f30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding layer from the model (i.e. first layer)\n",
    "embedding_layer = model.layers[0]\n",
    "\n",
    "# Get the weights of the embedding layer\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Print the shape. Expected is (vocab_size, embedding_dim)\n",
    "print(embedding_weights.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00886036,  0.02590651, -0.02247885, ...,  0.04109669,\n",
       "         0.00868085,  0.03497772],\n",
       "       [ 0.05131505,  0.08854441, -0.01564756, ...,  0.045674  ,\n",
       "        -0.0501706 ,  0.02132497],\n",
       "       [ 0.01393   ,  0.15215446, -0.02950421, ...,  0.09795719,\n",
       "        -0.06562466, -0.03304426],\n",
       "       ...,\n",
       "       [-0.05536873, -0.10137179,  0.01904162, ...,  0.00632618,\n",
       "        -0.0512451 , -0.02693657],\n",
       "       [-0.08813401, -0.10038247,  0.01387585, ...,  0.00210823,\n",
       "        -0.0540947 , -0.05190746],\n",
       "       [-0.03993828, -0.15481925, -0.08906949, ...,  0.05859702,\n",
       "        -0.04466905,  0.03263794]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index-word dictionary\n",
    "reverse_word_index = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<OOV>',\n",
       " 2: 'the',\n",
       " 3: 'and',\n",
       " 4: 'a',\n",
       " 5: 'of',\n",
       " 6: 'to',\n",
       " 7: 'is',\n",
       " 8: 'br',\n",
       " 9: 'in',\n",
       " 10: 'it',\n",
       " 11: 'i',\n",
       " 12: 'this',\n",
       " 13: 'that',\n",
       " 14: 'was',\n",
       " 15: 'as',\n",
       " 16: 'for',\n",
       " 17: 'with',\n",
       " 18: 'movie',\n",
       " 19: 'but',\n",
       " 20: 'film',\n",
       " 21: 'on',\n",
       " 22: 'not',\n",
       " 23: 'you',\n",
       " 24: 'are',\n",
       " 25: 'his',\n",
       " 26: 'have',\n",
       " 27: 'he',\n",
       " 28: 'be',\n",
       " 29: 'one',\n",
       " 30: 'all',\n",
       " 31: 'at',\n",
       " 32: 'by',\n",
       " 33: 'an',\n",
       " 34: 'they',\n",
       " 35: 'who',\n",
       " 36: 'so',\n",
       " 37: 'from',\n",
       " 38: 'like',\n",
       " 39: 'her',\n",
       " 40: 'or',\n",
       " 41: 'just',\n",
       " 42: 'about',\n",
       " 43: \"it's\",\n",
       " 44: 'out',\n",
       " 45: 'if',\n",
       " 46: 'has',\n",
       " 47: 'some',\n",
       " 48: 'there',\n",
       " 49: 'what',\n",
       " 50: 'good',\n",
       " 51: 'more',\n",
       " 52: 'when',\n",
       " 53: 'very',\n",
       " 54: 'up',\n",
       " 55: 'no',\n",
       " 56: 'time',\n",
       " 57: 'she',\n",
       " 58: 'even',\n",
       " 59: 'my',\n",
       " 60: 'would',\n",
       " 61: 'which',\n",
       " 62: 'only',\n",
       " 63: 'story',\n",
       " 64: 'really',\n",
       " 65: 'see',\n",
       " 66: 'their',\n",
       " 67: 'had',\n",
       " 68: 'can',\n",
       " 69: 'were',\n",
       " 70: 'me',\n",
       " 71: 'well',\n",
       " 72: 'than',\n",
       " 73: 'we',\n",
       " 74: 'much',\n",
       " 75: 'been',\n",
       " 76: 'bad',\n",
       " 77: 'get',\n",
       " 78: 'will',\n",
       " 79: 'do',\n",
       " 80: 'also',\n",
       " 81: 'into',\n",
       " 82: 'people',\n",
       " 83: 'other',\n",
       " 84: 'first',\n",
       " 85: 'great',\n",
       " 86: 'because',\n",
       " 87: 'how',\n",
       " 88: 'him',\n",
       " 89: 'most',\n",
       " 90: \"don't\",\n",
       " 91: 'made',\n",
       " 92: 'its',\n",
       " 93: 'then',\n",
       " 94: 'way',\n",
       " 95: 'make',\n",
       " 96: 'them',\n",
       " 97: 'too',\n",
       " 98: 'could',\n",
       " 99: 'any',\n",
       " 100: 'movies',\n",
       " 101: 'after',\n",
       " 102: 'think',\n",
       " 103: 'characters',\n",
       " 104: 'watch',\n",
       " 105: 'two',\n",
       " 106: 'films',\n",
       " 107: 'character',\n",
       " 108: 'seen',\n",
       " 109: 'many',\n",
       " 110: 'being',\n",
       " 111: 'life',\n",
       " 112: 'plot',\n",
       " 113: 'never',\n",
       " 114: 'acting',\n",
       " 115: 'little',\n",
       " 116: 'best',\n",
       " 117: 'love',\n",
       " 118: 'over',\n",
       " 119: 'where',\n",
       " 120: 'did',\n",
       " 121: 'show',\n",
       " 122: 'know',\n",
       " 123: 'off',\n",
       " 124: 'ever',\n",
       " 125: 'does',\n",
       " 126: 'better',\n",
       " 127: 'your',\n",
       " 128: 'end',\n",
       " 129: 'still',\n",
       " 130: 'man',\n",
       " 131: 'here',\n",
       " 132: 'these',\n",
       " 133: 'say',\n",
       " 134: 'scene',\n",
       " 135: 'while',\n",
       " 136: 'why',\n",
       " 137: 'scenes',\n",
       " 138: 'go',\n",
       " 139: 'such',\n",
       " 140: 'something',\n",
       " 141: 'through',\n",
       " 142: 'should',\n",
       " 143: 'back',\n",
       " 144: \"i'm\",\n",
       " 145: 'real',\n",
       " 146: 'those',\n",
       " 147: 'watching',\n",
       " 148: 'now',\n",
       " 149: 'though',\n",
       " 150: \"doesn't\",\n",
       " 151: 'years',\n",
       " 152: 'old',\n",
       " 153: 'thing',\n",
       " 154: 'actors',\n",
       " 155: 'work',\n",
       " 156: '10',\n",
       " 157: 'before',\n",
       " 158: 'another',\n",
       " 159: \"didn't\",\n",
       " 160: 'new',\n",
       " 161: 'funny',\n",
       " 162: 'nothing',\n",
       " 163: 'actually',\n",
       " 164: 'makes',\n",
       " 165: 'director',\n",
       " 166: 'look',\n",
       " 167: 'find',\n",
       " 168: 'going',\n",
       " 169: 'few',\n",
       " 170: 'same',\n",
       " 171: 'part',\n",
       " 172: 'again',\n",
       " 173: 'every',\n",
       " 174: 'lot',\n",
       " 175: 'cast',\n",
       " 176: 'us',\n",
       " 177: 'quite',\n",
       " 178: 'down',\n",
       " 179: 'want',\n",
       " 180: 'world',\n",
       " 181: 'things',\n",
       " 182: 'pretty',\n",
       " 183: 'young',\n",
       " 184: 'seems',\n",
       " 185: 'around',\n",
       " 186: 'got',\n",
       " 187: 'horror',\n",
       " 188: 'however',\n",
       " 189: \"can't\",\n",
       " 190: 'fact',\n",
       " 191: 'take',\n",
       " 192: 'big',\n",
       " 193: 'enough',\n",
       " 194: 'long',\n",
       " 195: 'thought',\n",
       " 196: \"that's\",\n",
       " 197: 'both',\n",
       " 198: 'between',\n",
       " 199: 'series',\n",
       " 200: 'give',\n",
       " 201: 'may',\n",
       " 202: 'original',\n",
       " 203: 'own',\n",
       " 204: 'action',\n",
       " 205: \"i've\",\n",
       " 206: 'right',\n",
       " 207: 'without',\n",
       " 208: 'always',\n",
       " 209: 'times',\n",
       " 210: 'comedy',\n",
       " 211: 'point',\n",
       " 212: 'gets',\n",
       " 213: 'must',\n",
       " 214: 'come',\n",
       " 215: 'role',\n",
       " 216: \"isn't\",\n",
       " 217: 'saw',\n",
       " 218: 'almost',\n",
       " 219: 'interesting',\n",
       " 220: 'least',\n",
       " 221: 'family',\n",
       " 222: 'done',\n",
       " 223: \"there's\",\n",
       " 224: 'whole',\n",
       " 225: 'bit',\n",
       " 226: 'music',\n",
       " 227: 'script',\n",
       " 228: 'far',\n",
       " 229: 'making',\n",
       " 230: 'anything',\n",
       " 231: 'guy',\n",
       " 232: 'minutes',\n",
       " 233: 'feel',\n",
       " 234: 'last',\n",
       " 235: 'since',\n",
       " 236: 'might',\n",
       " 237: 'performance',\n",
       " 238: \"he's\",\n",
       " 239: '2',\n",
       " 240: 'probably',\n",
       " 241: 'kind',\n",
       " 242: 'am',\n",
       " 243: 'away',\n",
       " 244: 'yet',\n",
       " 245: 'rather',\n",
       " 246: 'tv',\n",
       " 247: 'worst',\n",
       " 248: 'girl',\n",
       " 249: 'day',\n",
       " 250: 'sure',\n",
       " 251: 'fun',\n",
       " 252: 'hard',\n",
       " 253: 'woman',\n",
       " 254: 'played',\n",
       " 255: 'each',\n",
       " 256: 'found',\n",
       " 257: 'anyone',\n",
       " 258: 'having',\n",
       " 259: 'especially',\n",
       " 260: 'although',\n",
       " 261: 'our',\n",
       " 262: 'course',\n",
       " 263: 'believe',\n",
       " 264: 'comes',\n",
       " 265: 'looking',\n",
       " 266: 'screen',\n",
       " 267: 'trying',\n",
       " 268: 'set',\n",
       " 269: 'goes',\n",
       " 270: 'looks',\n",
       " 271: 'place',\n",
       " 272: 'book',\n",
       " 273: 'different',\n",
       " 274: 'put',\n",
       " 275: 'ending',\n",
       " 276: 'money',\n",
       " 277: 'maybe',\n",
       " 278: 'once',\n",
       " 279: 'sense',\n",
       " 280: 'reason',\n",
       " 281: 'true',\n",
       " 282: 'actor',\n",
       " 283: 'everything',\n",
       " 284: \"wasn't\",\n",
       " 285: 'shows',\n",
       " 286: 'dvd',\n",
       " 287: 'three',\n",
       " 288: 'worth',\n",
       " 289: 'year',\n",
       " 290: 'job',\n",
       " 291: 'main',\n",
       " 292: 'someone',\n",
       " 293: 'together',\n",
       " 294: 'watched',\n",
       " 295: 'play',\n",
       " 296: 'american',\n",
       " 297: 'plays',\n",
       " 298: '1',\n",
       " 299: 'said',\n",
       " 300: 'effects',\n",
       " 301: 'later',\n",
       " 302: 'takes',\n",
       " 303: 'instead',\n",
       " 304: 'seem',\n",
       " 305: 'beautiful',\n",
       " 306: 'john',\n",
       " 307: 'himself',\n",
       " 308: 'version',\n",
       " 309: 'audience',\n",
       " 310: 'high',\n",
       " 311: 'house',\n",
       " 312: 'night',\n",
       " 313: 'during',\n",
       " 314: 'everyone',\n",
       " 315: 'left',\n",
       " 316: 'special',\n",
       " 317: 'seeing',\n",
       " 318: 'half',\n",
       " 319: 'excellent',\n",
       " 320: 'wife',\n",
       " 321: 'star',\n",
       " 322: 'shot',\n",
       " 323: 'war',\n",
       " 324: 'idea',\n",
       " 325: 'nice',\n",
       " 326: 'black',\n",
       " 327: 'less',\n",
       " 328: 'mind',\n",
       " 329: 'simply',\n",
       " 330: 'read',\n",
       " 331: 'second',\n",
       " 332: 'else',\n",
       " 333: \"you're\",\n",
       " 334: 'father',\n",
       " 335: 'fan',\n",
       " 336: 'help',\n",
       " 337: 'poor',\n",
       " 338: 'completely',\n",
       " 339: 'death',\n",
       " 340: '3',\n",
       " 341: 'used',\n",
       " 342: 'home',\n",
       " 343: 'either',\n",
       " 344: 'short',\n",
       " 345: 'line',\n",
       " 346: 'given',\n",
       " 347: 'men',\n",
       " 348: 'top',\n",
       " 349: 'dead',\n",
       " 350: 'budget',\n",
       " 351: 'try',\n",
       " 352: 'performances',\n",
       " 353: 'wrong',\n",
       " 354: 'classic',\n",
       " 355: 'boring',\n",
       " 356: 'enjoy',\n",
       " 357: 'need',\n",
       " 358: 'rest',\n",
       " 359: 'use',\n",
       " 360: 'hollywood',\n",
       " 361: 'kids',\n",
       " 362: 'low',\n",
       " 363: 'production',\n",
       " 364: 'until',\n",
       " 365: 'along',\n",
       " 366: 'full',\n",
       " 367: 'friends',\n",
       " 368: 'camera',\n",
       " 369: 'truly',\n",
       " 370: 'women',\n",
       " 371: 'awful',\n",
       " 372: 'video',\n",
       " 373: 'next',\n",
       " 374: 'tell',\n",
       " 375: 'remember',\n",
       " 376: 'couple',\n",
       " 377: 'stupid',\n",
       " 378: 'start',\n",
       " 379: 'stars',\n",
       " 380: 'perhaps',\n",
       " 381: 'mean',\n",
       " 382: 'sex',\n",
       " 383: 'came',\n",
       " 384: 'recommend',\n",
       " 385: 'let',\n",
       " 386: 'moments',\n",
       " 387: 'wonderful',\n",
       " 388: 'episode',\n",
       " 389: 'understand',\n",
       " 390: 'small',\n",
       " 391: 'face',\n",
       " 392: 'terrible',\n",
       " 393: 'school',\n",
       " 394: 'playing',\n",
       " 395: 'getting',\n",
       " 396: 'written',\n",
       " 397: 'often',\n",
       " 398: 'doing',\n",
       " 399: 'keep',\n",
       " 400: 'early',\n",
       " 401: 'name',\n",
       " 402: 'perfect',\n",
       " 403: 'style',\n",
       " 404: 'human',\n",
       " 405: 'definitely',\n",
       " 406: 'others',\n",
       " 407: 'gives',\n",
       " 408: 'itself',\n",
       " 409: 'lines',\n",
       " 410: 'live',\n",
       " 411: 'become',\n",
       " 412: 'person',\n",
       " 413: 'dialogue',\n",
       " 414: 'lost',\n",
       " 415: 'finally',\n",
       " 416: 'piece',\n",
       " 417: 'head',\n",
       " 418: 'case',\n",
       " 419: 'felt',\n",
       " 420: 'yes',\n",
       " 421: 'liked',\n",
       " 422: 'supposed',\n",
       " 423: 'title',\n",
       " 424: \"couldn't\",\n",
       " 425: 'absolutely',\n",
       " 426: 'white',\n",
       " 427: 'against',\n",
       " 428: 'boy',\n",
       " 429: 'picture',\n",
       " 430: 'sort',\n",
       " 431: 'worse',\n",
       " 432: 'certainly',\n",
       " 433: 'went',\n",
       " 434: 'entire',\n",
       " 435: 'cinema',\n",
       " 436: 'waste',\n",
       " 437: 'problem',\n",
       " 438: 'hope',\n",
       " 439: \"she's\",\n",
       " 440: 'entertaining',\n",
       " 441: 'mr',\n",
       " 442: 'overall',\n",
       " 443: 'evil',\n",
       " 444: 'called',\n",
       " 445: 'loved',\n",
       " 446: 'based',\n",
       " 447: 'oh',\n",
       " 448: 'several',\n",
       " 449: 'fans',\n",
       " 450: 'mother',\n",
       " 451: 'drama',\n",
       " 452: 'beginning',\n",
       " 453: 'killer',\n",
       " 454: 'lives',\n",
       " 455: '5',\n",
       " 456: 'direction',\n",
       " 457: 'care',\n",
       " 458: 'becomes',\n",
       " 459: 'already',\n",
       " 460: 'laugh',\n",
       " 461: 'example',\n",
       " 462: 'friend',\n",
       " 463: 'dark',\n",
       " 464: 'despite',\n",
       " 465: 'under',\n",
       " 466: 'seemed',\n",
       " 467: 'throughout',\n",
       " 468: '4',\n",
       " 469: 'turn',\n",
       " 470: 'unfortunately',\n",
       " 471: 'wanted',\n",
       " 472: \"i'd\",\n",
       " 473: '\\x96',\n",
       " 474: 'children',\n",
       " 475: 'final',\n",
       " 476: 'fine',\n",
       " 477: 'history',\n",
       " 478: 'amazing',\n",
       " 479: 'sound',\n",
       " 480: 'guess',\n",
       " 481: 'heart',\n",
       " 482: 'totally',\n",
       " 483: 'humor',\n",
       " 484: 'lead',\n",
       " 485: 'writing',\n",
       " 486: 'michael',\n",
       " 487: 'quality',\n",
       " 488: \"you'll\",\n",
       " 489: 'close',\n",
       " 490: 'son',\n",
       " 491: 'wants',\n",
       " 492: 'guys',\n",
       " 493: 'works',\n",
       " 494: 'behind',\n",
       " 495: 'tries',\n",
       " 496: 'art',\n",
       " 497: 'side',\n",
       " 498: 'game',\n",
       " 499: 'past',\n",
       " 500: 'able',\n",
       " 501: 'b',\n",
       " 502: 'days',\n",
       " 503: 'turns',\n",
       " 504: \"they're\",\n",
       " 505: 'child',\n",
       " 506: 'hand',\n",
       " 507: 'flick',\n",
       " 508: 'enjoyed',\n",
       " 509: 'act',\n",
       " 510: 'genre',\n",
       " 511: 'town',\n",
       " 512: 'favorite',\n",
       " 513: 'soon',\n",
       " 514: 'kill',\n",
       " 515: 'starts',\n",
       " 516: 'sometimes',\n",
       " 517: 'gave',\n",
       " 518: 'car',\n",
       " 519: 'run',\n",
       " 520: 'late',\n",
       " 521: 'eyes',\n",
       " 522: 'etc',\n",
       " 523: 'actress',\n",
       " 524: 'directed',\n",
       " 525: 'horrible',\n",
       " 526: \"won't\",\n",
       " 527: 'brilliant',\n",
       " 528: 'viewer',\n",
       " 529: 'parts',\n",
       " 530: 'themselves',\n",
       " 531: 'self',\n",
       " 532: 'hour',\n",
       " 533: 'expect',\n",
       " 534: 'thinking',\n",
       " 535: 'stories',\n",
       " 536: 'stuff',\n",
       " 537: 'girls',\n",
       " 538: 'obviously',\n",
       " 539: 'blood',\n",
       " 540: 'decent',\n",
       " 541: 'city',\n",
       " 542: 'voice',\n",
       " 543: 'highly',\n",
       " 544: 'myself',\n",
       " 545: 'feeling',\n",
       " 546: 'fight',\n",
       " 547: 'except',\n",
       " 548: 'slow',\n",
       " 549: 'matter',\n",
       " 550: 'type',\n",
       " 551: 'anyway',\n",
       " 552: 'kid',\n",
       " 553: 'roles',\n",
       " 554: 'killed',\n",
       " 555: 'heard',\n",
       " 556: 'age',\n",
       " 557: 'says',\n",
       " 558: 'god',\n",
       " 559: 'moment',\n",
       " 560: 'took',\n",
       " 561: 'leave',\n",
       " 562: 'writer',\n",
       " 563: 'strong',\n",
       " 564: 'cannot',\n",
       " 565: 'violence',\n",
       " 566: 'police',\n",
       " 567: 'hit',\n",
       " 568: 'stop',\n",
       " 569: 'happens',\n",
       " 570: 'particularly',\n",
       " 571: 'known',\n",
       " 572: 'happened',\n",
       " 573: 'involved',\n",
       " 574: 'extremely',\n",
       " 575: 'daughter',\n",
       " 576: 'obvious',\n",
       " 577: 'told',\n",
       " 578: 'chance',\n",
       " 579: 'living',\n",
       " 580: 'coming',\n",
       " 581: 'lack',\n",
       " 582: 'experience',\n",
       " 583: 'alone',\n",
       " 584: 'including',\n",
       " 585: \"wouldn't\",\n",
       " 586: 'murder',\n",
       " 587: 'attempt',\n",
       " 588: 's',\n",
       " 589: 'james',\n",
       " 590: 'please',\n",
       " 591: 'happen',\n",
       " 592: 'wonder',\n",
       " 593: 'crap',\n",
       " 594: 'ago',\n",
       " 595: \"film's\",\n",
       " 596: 'brother',\n",
       " 597: 'gore',\n",
       " 598: 'complete',\n",
       " 599: 'none',\n",
       " 600: 'interest',\n",
       " 601: 'score',\n",
       " 602: 'group',\n",
       " 603: 'cut',\n",
       " 604: 'simple',\n",
       " 605: 'save',\n",
       " 606: 'ok',\n",
       " 607: 'hell',\n",
       " 608: 'looked',\n",
       " 609: 'number',\n",
       " 610: 'career',\n",
       " 611: 'song',\n",
       " 612: 'possible',\n",
       " 613: 'seriously',\n",
       " 614: 'annoying',\n",
       " 615: 'shown',\n",
       " 616: 'exactly',\n",
       " 617: 'sad',\n",
       " 618: 'running',\n",
       " 619: 'musical',\n",
       " 620: 'serious',\n",
       " 621: 'yourself',\n",
       " 622: 'taken',\n",
       " 623: 'released',\n",
       " 624: 'whose',\n",
       " 625: 'david',\n",
       " 626: 'cinematography',\n",
       " 627: 'scary',\n",
       " 628: 'ends',\n",
       " 629: 'usually',\n",
       " 630: 'hero',\n",
       " 631: 'english',\n",
       " 632: 'hours',\n",
       " 633: 'reality',\n",
       " 634: 'opening',\n",
       " 635: \"i'll\",\n",
       " 636: 'today',\n",
       " 637: 'light',\n",
       " 638: 'across',\n",
       " 639: 'jokes',\n",
       " 640: 'hilarious',\n",
       " 641: 'somewhat',\n",
       " 642: 'usual',\n",
       " 643: 'ridiculous',\n",
       " 644: 'body',\n",
       " 645: 'cool',\n",
       " 646: 'started',\n",
       " 647: 'level',\n",
       " 648: 'view',\n",
       " 649: 'relationship',\n",
       " 650: 'change',\n",
       " 651: 'opinion',\n",
       " 652: 'happy',\n",
       " 653: 'middle',\n",
       " 654: 'taking',\n",
       " 655: 'wish',\n",
       " 656: 'finds',\n",
       " 657: 'husband',\n",
       " 658: 'order',\n",
       " 659: 'saying',\n",
       " 660: 'talking',\n",
       " 661: 'shots',\n",
       " 662: 'ones',\n",
       " 663: 'documentary',\n",
       " 664: 'huge',\n",
       " 665: 'novel',\n",
       " 666: 'mostly',\n",
       " 667: 'female',\n",
       " 668: 'robert',\n",
       " 669: 'power',\n",
       " 670: 'episodes',\n",
       " 671: 'room',\n",
       " 672: 'important',\n",
       " 673: 'rating',\n",
       " 674: 'talent',\n",
       " 675: 'five',\n",
       " 676: 'major',\n",
       " 677: 'turned',\n",
       " 678: 'strange',\n",
       " 679: 'word',\n",
       " 680: 'modern',\n",
       " 681: 'call',\n",
       " 682: 'apparently',\n",
       " 683: 'disappointed',\n",
       " 684: 'single',\n",
       " 685: 'events',\n",
       " 686: 'due',\n",
       " 687: 'four',\n",
       " 688: 'songs',\n",
       " 689: 'basically',\n",
       " 690: 'attention',\n",
       " 691: '7',\n",
       " 692: 'knows',\n",
       " 693: 'clearly',\n",
       " 694: 'supporting',\n",
       " 695: 'knew',\n",
       " 696: 'comic',\n",
       " 697: 'non',\n",
       " 698: 'british',\n",
       " 699: 'television',\n",
       " 700: 'fast',\n",
       " 701: 'earth',\n",
       " 702: 'country',\n",
       " 703: 'cheap',\n",
       " 704: 'class',\n",
       " 705: 'future',\n",
       " 706: 'silly',\n",
       " 707: 'thriller',\n",
       " 708: '8',\n",
       " 709: 'king',\n",
       " 710: 'problems',\n",
       " 711: \"aren't\",\n",
       " 712: 'easily',\n",
       " 713: 'words',\n",
       " 714: 'tells',\n",
       " 715: 'miss',\n",
       " 716: 'jack',\n",
       " 717: 'local',\n",
       " 718: 'sequence',\n",
       " 719: 'bring',\n",
       " 720: 'entertainment',\n",
       " 721: 'paul',\n",
       " 722: 'beyond',\n",
       " 723: 'upon',\n",
       " 724: 'whether',\n",
       " 725: 'predictable',\n",
       " 726: 'moving',\n",
       " 727: 'straight',\n",
       " 728: 'sets',\n",
       " 729: 'similar',\n",
       " 730: 'romantic',\n",
       " 731: 'review',\n",
       " 732: 'oscar',\n",
       " 733: 'falls',\n",
       " 734: 'mystery',\n",
       " 735: 'enjoyable',\n",
       " 736: 'needs',\n",
       " 737: 'rock',\n",
       " 738: 'appears',\n",
       " 739: 'talk',\n",
       " 740: 'george',\n",
       " 741: 'giving',\n",
       " 742: 'eye',\n",
       " 743: 'within',\n",
       " 744: 'richard',\n",
       " 745: 'ten',\n",
       " 746: 'animation',\n",
       " 747: 'message',\n",
       " 748: 'near',\n",
       " 749: 'theater',\n",
       " 750: 'above',\n",
       " 751: 'dull',\n",
       " 752: 'nearly',\n",
       " 753: 'sequel',\n",
       " 754: 'theme',\n",
       " 755: 'points',\n",
       " 756: \"'\",\n",
       " 757: 'stand',\n",
       " 758: 'mention',\n",
       " 759: 'lady',\n",
       " 760: 'bunch',\n",
       " 761: 'add',\n",
       " 762: 'feels',\n",
       " 763: 'herself',\n",
       " 764: 'release',\n",
       " 765: 'red',\n",
       " 766: 'team',\n",
       " 767: 'storyline',\n",
       " 768: 'surprised',\n",
       " 769: 'ways',\n",
       " 770: 'named',\n",
       " 771: 'using',\n",
       " 772: \"haven't\",\n",
       " 773: 'lots',\n",
       " 774: 'easy',\n",
       " 775: 'fantastic',\n",
       " 776: 'begins',\n",
       " 777: 'actual',\n",
       " 778: 'working',\n",
       " 779: 'effort',\n",
       " 780: 'york',\n",
       " 781: 'die',\n",
       " 782: 'hate',\n",
       " 783: 'french',\n",
       " 784: 'tale',\n",
       " 785: 'minute',\n",
       " 786: 'stay',\n",
       " 787: '9',\n",
       " 788: 'clear',\n",
       " 789: 'feature',\n",
       " 790: 'elements',\n",
       " 791: 'among',\n",
       " 792: 'follow',\n",
       " 793: 're',\n",
       " 794: 'comments',\n",
       " 795: 'viewers',\n",
       " 796: 'avoid',\n",
       " 797: 'sister',\n",
       " 798: 'typical',\n",
       " 799: 'showing',\n",
       " 800: 'editing',\n",
       " 801: \"what's\",\n",
       " 802: 'famous',\n",
       " 803: 'tried',\n",
       " 804: 'sorry',\n",
       " 805: 'fall',\n",
       " 806: 'dialog',\n",
       " 807: 'check',\n",
       " 808: 'period',\n",
       " 809: 'form',\n",
       " 810: 'season',\n",
       " 811: 'certain',\n",
       " 812: 'filmed',\n",
       " 813: 'weak',\n",
       " 814: 'soundtrack',\n",
       " 815: 'means',\n",
       " 816: 'material',\n",
       " 817: 'buy',\n",
       " 818: 'somehow',\n",
       " 819: 'realistic',\n",
       " 820: 'figure',\n",
       " 821: 'crime',\n",
       " 822: 'gone',\n",
       " 823: 'doubt',\n",
       " 824: 'peter',\n",
       " 825: 'tom',\n",
       " 826: 'kept',\n",
       " 827: 'viewing',\n",
       " 828: 't',\n",
       " 829: 'general',\n",
       " 830: 'leads',\n",
       " 831: 'greatest',\n",
       " 832: 'space',\n",
       " 833: 'lame',\n",
       " 834: 'suspense',\n",
       " 835: 'dance',\n",
       " 836: 'brought',\n",
       " 837: 'imagine',\n",
       " 838: 'third',\n",
       " 839: 'atmosphere',\n",
       " 840: 'hear',\n",
       " 841: 'particular',\n",
       " 842: 'sequences',\n",
       " 843: 'whatever',\n",
       " 844: 'parents',\n",
       " 845: 'move',\n",
       " 846: 'lee',\n",
       " 847: 'indeed',\n",
       " 848: 'rent',\n",
       " 849: 'de',\n",
       " 850: 'eventually',\n",
       " 851: 'learn',\n",
       " 852: 'note',\n",
       " 853: 'wait',\n",
       " 854: 'deal',\n",
       " 855: 'forget',\n",
       " 856: 'reviews',\n",
       " 857: 'average',\n",
       " 858: 'japanese',\n",
       " 859: 'sexual',\n",
       " 860: 'poorly',\n",
       " 861: 'okay',\n",
       " 862: 'premise',\n",
       " 863: 'zombie',\n",
       " 864: 'surprise',\n",
       " 865: 'believable',\n",
       " 866: 'stage',\n",
       " 867: 'sit',\n",
       " 868: 'possibly',\n",
       " 869: \"who's\",\n",
       " 870: 'decided',\n",
       " 871: 'expected',\n",
       " 872: \"you've\",\n",
       " 873: 'subject',\n",
       " 874: 'nature',\n",
       " 875: 'became',\n",
       " 876: 'difficult',\n",
       " 877: 'free',\n",
       " 878: 'killing',\n",
       " 879: 'screenplay',\n",
       " 880: 'truth',\n",
       " 881: 'romance',\n",
       " 882: 'dr',\n",
       " 883: 'nor',\n",
       " 884: 'reading',\n",
       " 885: 'needed',\n",
       " 886: 'question',\n",
       " 887: 'leaves',\n",
       " 888: 'street',\n",
       " 889: '20',\n",
       " 890: 'meets',\n",
       " 891: 'hot',\n",
       " 892: 'unless',\n",
       " 893: 'begin',\n",
       " 894: 'baby',\n",
       " 895: 'superb',\n",
       " 896: 'credits',\n",
       " 897: 'otherwise',\n",
       " 898: 'imdb',\n",
       " 899: 'write',\n",
       " 900: 'shame',\n",
       " 901: \"let's\",\n",
       " 902: 'situation',\n",
       " 903: 'dramatic',\n",
       " 904: 'memorable',\n",
       " 905: 'directors',\n",
       " 906: 'earlier',\n",
       " 907: 'badly',\n",
       " 908: 'disney',\n",
       " 909: 'meet',\n",
       " 910: 'open',\n",
       " 911: 'dog',\n",
       " 912: 'male',\n",
       " 913: 'weird',\n",
       " 914: 'joe',\n",
       " 915: 'acted',\n",
       " 916: 'forced',\n",
       " 917: 'emotional',\n",
       " 918: 'sci',\n",
       " 919: 'laughs',\n",
       " 920: 'older',\n",
       " 921: 'realize',\n",
       " 922: 'fi',\n",
       " 923: 'dream',\n",
       " 924: 'society',\n",
       " 925: 'interested',\n",
       " 926: 'writers',\n",
       " 927: 'comment',\n",
       " 928: 'forward',\n",
       " 929: 'footage',\n",
       " 930: 'crazy',\n",
       " 931: 'deep',\n",
       " 932: 'whom',\n",
       " 933: 'sounds',\n",
       " 934: 'beauty',\n",
       " 935: 'america',\n",
       " 936: 'plus',\n",
       " 937: 'fantasy',\n",
       " 938: 'directing',\n",
       " 939: 'keeps',\n",
       " 940: 'development',\n",
       " 941: 'ask',\n",
       " 942: 'features',\n",
       " 943: 'air',\n",
       " 944: 'quickly',\n",
       " 945: 'mess',\n",
       " 946: 'creepy',\n",
       " 947: 'perfectly',\n",
       " 948: 'towards',\n",
       " 949: 'mark',\n",
       " 950: 'worked',\n",
       " 951: 'box',\n",
       " 952: 'cheesy',\n",
       " 953: 'unique',\n",
       " 954: 'setting',\n",
       " 955: 'hands',\n",
       " 956: 'plenty',\n",
       " 957: 'brings',\n",
       " 958: 'previous',\n",
       " 959: 'result',\n",
       " 960: 'e',\n",
       " 961: 'effect',\n",
       " 962: 'total',\n",
       " 963: 'personal',\n",
       " 964: 'incredibly',\n",
       " 965: 'rate',\n",
       " 966: 'fire',\n",
       " 967: 'monster',\n",
       " 968: 'business',\n",
       " 969: 'leading',\n",
       " 970: 'apart',\n",
       " 971: 'casting',\n",
       " 972: 'admit',\n",
       " 973: 'appear',\n",
       " 974: 'joke',\n",
       " 975: 'background',\n",
       " 976: 'powerful',\n",
       " 977: 'telling',\n",
       " 978: 'meant',\n",
       " 979: 'girlfriend',\n",
       " 980: 'christmas',\n",
       " 981: 'present',\n",
       " 982: 'hardly',\n",
       " 983: 'potential',\n",
       " 984: 'battle',\n",
       " 985: 'create',\n",
       " 986: 'bill',\n",
       " 987: 'break',\n",
       " 988: 'pay',\n",
       " 989: 'masterpiece',\n",
       " 990: 'dumb',\n",
       " 991: 'return',\n",
       " 992: 'political',\n",
       " 993: 'gay',\n",
       " 994: 'fails',\n",
       " 995: 'fighting',\n",
       " 996: 'various',\n",
       " 997: 'era',\n",
       " 998: 'portrayed',\n",
       " 999: 'secret',\n",
       " 1000: 'cop',\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Open writeable files\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "# Initialize the loop. Start counting at `1` because `0` is just for the padding\n",
    "for word_num in range(1, vocab_size):\n",
    "\n",
    "  # Get the word associated at the current index\n",
    "  word_name = reverse_word_index[word_num]\n",
    "\n",
    "  # Get the embedding weights associated with the current index\n",
    "  word_embedding = embedding_weights[word_num]\n",
    "\n",
    "  # Write the word name\n",
    "  out_m.write(word_name + \"\\n\")\n",
    "\n",
    "  # Write the word embedding\n",
    "  out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "\n",
    "# Close the files\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad528b108aa1fc0ce0087702c1245bacb25467d3e2e9e3cbe2202ae00c9ee454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
